{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import neptune\n",
    "import torch\n",
    "import json\n",
    "import copy\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "import zipfile\n",
    "import sys\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune.init('iliaavilov/SIBUR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_index = 'SIB-675'\n",
    "n_trial = 19\n",
    "target = 'iC4H10'\n",
    "\n",
    "def get_exp_data(exp_index, n_trial):\n",
    "    exp = neptune.project.get_experiments(exp_index)[0]\n",
    "\n",
    "    channel_names = ['current_params']\n",
    "    channels_data = {}\n",
    "    channels_by_name = exp.get_channels()\n",
    "    for channel_name in channel_names:\n",
    "        channel_id = channels_by_name[channel_name].id\n",
    "\n",
    "        channels_data[channel_name] = pd.read_csv(\n",
    "            exp._backend.get_channel_points_csv(exp, channel_id),\n",
    "            header=None,\n",
    "            dtype=str\n",
    "        )\n",
    "\n",
    "    values = exp._backend.get_channel_points_csv(exp, channel_id).getvalue()\n",
    "    data = pd.DataFrame(values.split('\\n'))\n",
    "    data = data.iloc[:-1]\n",
    "    data[0] = data[0].apply(lambda x: json.loads('{' + x.split(',{')[1].replace(\"'\", '\"')))\n",
    "    data.columns = ['params']\n",
    "#    iterations = exp.get_numeric_channels_values('iterations').drop('x', axis = 'columns')\n",
    "#    data = pd.concat([data, iterations], axis = 'columns')\n",
    "\n",
    "    data_trial = data.iloc[n_trial, ]\n",
    "    params_trial = data_trial['params']\n",
    "#    params_trial['n_estimators'] = round(data_trial['iterations'])\n",
    "\n",
    "    exp.download_sources()\n",
    "    with zipfile.ZipFile(\"source.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall('')\n",
    "    \n",
    "    return(params_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_trial = get_exp_data(exp_index, n_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('source/')\n",
    "import preprocessing\n",
    "from NN import simple_torchpl\n",
    "from load_data import load\n",
    "from pl_framework import nn_training\n",
    "from cv import get_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "random_state = 54321\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_targets, test_features = load(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = get_indices(train_targets, [(pd.to_datetime('2020-03-01 00:00:00'), pd.to_datetime('2020-03-15 00:00:00')),\n",
    "                                 (pd.to_datetime('2020-03-15 00:00:00'), pd.to_datetime('2020-03-31 00:00:00')),\n",
    "                                 (pd.to_datetime('2020-03-31 00:00:00'), pd.to_datetime('2020-04-15 00:00:00'))\n",
    "                                ]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = test_features['timestamp'].values\n",
    "#train_targets = train_targets.drop('timestamp', axis = 'columns')\n",
    "#test_features = test_features.drop('timestamp', axis = 'columns')\n",
    "#train_features = train_features.drop('timestamp', axis = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказания с помощью sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = train_features.append(test_features, ignore_index = True)\n",
    "n_backs = []\n",
    "for feature in ['A_{}'.format(target)]:\n",
    "    n_backs.append(params_trial['n_back_'+feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = train_features.append(test_features, ignore_index = True)\n",
    "\n",
    "X, y_train, cv, params = preprocessing.preprocessing(all_features.copy(), \n",
    "                                                     train_targets.copy(), \n",
    "                                                     copy.deepcopy(cv), \n",
    "                                                     copy.deepcopy(params_trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:(train_features.shape[0] - max(n_backs))]\n",
    "X_test = X[(train_features.shape[0] -  max(n_backs)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train.shape)\n",
    "print(test_features.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = SGDRegressor(**params)\n",
    "LR.fit(X_train[cv[0][0]], y_train[cv[0][0]], sample_weight = 1/y_train[cv[0][0]])\n",
    "predictions = LR.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}.pickle'.format(target), 'wb') as f:\n",
    "    pickle.dump(LR, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('submission.csv') == True:\n",
    "    submission = pd.read_csv('submission.csv')\n",
    "    submission['B_' + target] = predictions\n",
    "    submission.to_csv('submission.csv', index = False)\n",
    "else:\n",
    "    submission = pd.DataFrame(columns = ['B_C2H6', 'B_C3H8', 'B_iC4H10', 'B_nC4H10'])\n",
    "    submission['timestamp'] = ts\n",
    "    submission['B_' + target] = predictions\n",
    "    submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказания с помощью lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = train_features.append(test_features, ignore_index = True)\n",
    "n_backs = []\n",
    "for feature in ['A_CH4', 'A_C2H6', 'A_C3H8', 'A_iC4H10', 'A_nC4H10', 'A_iC5H12', 'A_nC5H12', 'A_C6H14']:\n",
    "    n_backs.append(params_trial['n_back_'+feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_features = train_features.append(test_features, ignore_index = True)\n",
    "\n",
    "X, y_train, cv, params = preprocessing.preprocessing(all_features.copy(), \n",
    "                                                     train_targets.copy(), \n",
    "                                                     copy.deepcopy(cv), \n",
    "                                                     copy.deepcopy(params_trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:(train_features.shape[0] - max(n_backs))]\n",
    "X_test = X[(train_features.shape[0] -  max(n_backs)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train.shape)\n",
    "print(test_features.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(X_train[cv[-1][0], :], y_train[cv[-1][0]])\n",
    "test_data = lgb.Dataset(X_train[cv[-1][1], :], y_train[cv[-1][1]])\n",
    "\n",
    "def lgb_scoring(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    return 'loss', np.mean(np.abs((y_true - y_hat)/y_true)), False\n",
    "    \n",
    "test_model = lgb.train(params = params, train_set = train_data, verbose_eval = False)\n",
    "predictions = test_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('submission.csv') == True:\n",
    "    submission = pd.read_csv('submission.csv')\n",
    "    submission[target] = predictions\n",
    "    submission.to_csv('submission.csv', index = False)\n",
    "else:\n",
    "    submission = pd.DataFrame(columns = ['B_C2H6', 'B_C3H8', 'B_iC4H10', 'B_nC4H10'])\n",
    "    submission['timestamp'] = ts\n",
    "    submission[target] = predictions\n",
    "    submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neptune.project.get_experiments(exp_index)[0].log_metric('leaderboard_mape', 2.4408)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Предсказания с помощью нейронки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_features = train_features.append(test_features, ignore_index = True)\n",
    "n_backs = []\n",
    "for feature in ['A_CH4', 'A_C2H6', 'A_C3H8', 'A_iC4H10', 'A_nC4H10', 'A_iC5H12', 'A_nC5H12', 'A_C6H14']:\n",
    "    n_backs.append(params_trial['n_back_'+feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_features = train_features.append(test_features, ignore_index = True)\n",
    "\n",
    "X, y_train, cv, params = preprocessing.preprocessing(all_features.copy(), \n",
    "                                                     train_targets.copy(), \n",
    "                                                     copy.deepcopy(cv), \n",
    "                                                     copy.deepcopy(params_trial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = X[:(train_features.shape[0] - max(n_backs))]\n",
    "X_test = X[(train_features.shape[0] -  max(n_backs)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train.shape)\n",
    "print(test_features.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = params_trial['batch_size']\n",
    "params_trial.pop('batch_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_best_iter = params_trial['n_estimators']\n",
    "my_boiii = nn_training(simple_torchpl, X_train, y_train[[target]])\n",
    "my_boiii.train(min_epochs = mean_best_iter,\n",
    "               max_epochs = mean_best_iter,\n",
    "               model_params = params,\n",
    "               batch_size = batch_size,\n",
    "               fold = cv[0] ,\n",
    "               val_fold = False)\n",
    "\n",
    "my_model = my_boiii.trained_model\n",
    "my_model.eval()\n",
    "\n",
    "predictions = my_model(torch.from_numpy(X_test).float()).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if os.path.isfile('submission.csv') == True:\n",
    "    submission = pd.read_csv('submission.csv')\n",
    "    submission[target] = predictions\n",
    "    submission.to_csv('submission.csv', index = False)\n",
    "else:\n",
    "    submission = pd.DataFrame(columns = ['B_C2H6', 'B_C3H8', 'B_iC4H10', 'B_nC4H10'])\n",
    "    submission['timestamp'] = ts\n",
    "    submission[target] = predictions\n",
    "    submission.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#neptune.project.get_experiments(exp_index)[0].log_metric('leaderboard_mape', 2.8630)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "neptune": {
   "notebookId": "6eddcb21-b0fa-4073-a5c6-f26572800a8b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
